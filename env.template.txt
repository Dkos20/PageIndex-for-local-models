# Copy the content of this file into your .env and adjust values as needed.
# Select provider and configure the corresponding section below.

# Provider selection: 'openai' or 'custom'
CHATGPT_API_PROVIDER=custom

# Common: API key used for Authorization header if required by provider/proxy
# For OpenAI, set to your OpenAI key. For LiteLLM/local proxy, set to the token expected by the proxy (or leave empty if none)
CHATGPT_API_KEY=your_api_key_or_proxy_token

# Optional: default model name used when --model is not provided
# For OpenAI, examples: gpt-4o-2024-11-20
# For LiteLLM/custom, examples: mistral-small-latest, mistralai/Mistral-7B-Instruct-v0.2, qwen2.5:latest, etc.
CHATGPT_DEFAULT_MODEL=mistral-small-latest

# =========================
# OpenAI provider settings
# =========================
# If using OpenAI, set:
# CHATGPT_API_PROVIDER=openai
# CHATGPT_API_KEY=sk-...your_openai_key...
# (No need to configure CUSTOM_LLM_* variables)

# =========================
# Custom provider (LiteLLM/local) settings
# =========================
# Set CHATGPT_API_PROVIDER=custom and configure the endpoint below
# Example LiteLLM server: http://localhost:4000 forwarding to your chosen model
CUSTOM_LLM_BASE_URL=http://localhost:4000

# Path for chat completions. Most OpenAI-compatible proxies use /v1/chat/completions
CUSTOM_LLM_API_PATH=/v1/chat/completions

# Authorization header name and prefix. Many proxies accept standard Bearer tokens.
# If your proxy doesn't require auth, set CUSTOM_LLM_AUTH_HEADER empty and leave CHATGPT_API_KEY empty.
CUSTOM_LLM_AUTH_HEADER=Authorization
CUSTOM_LLM_API_KEY_PREFIX=Bearer 

# Notes:
# - The code expects OpenAI-compatible responses: {"choices":[{"message":{"content":"..."},"finish_reason":"stop"}]}.
# - If your proxy differs, align it to this schema (LiteLLM already matches).
# - Token counting uses a fallback encoding when model names are not recognized by tiktoken.
